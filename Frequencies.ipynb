{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "split-spoke",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The initial intuition that drives me to persue this project is that the DFT is inherently imperfect. The only way to precisely extract the exact frequencies in a signal is to run a DFT on an infinite number of buckets. And to perform a resolution between buckets to subtract nearby and aliased noise with an infinite number of recursions.\n",
    "\n",
    "The starting assumption when performing a DFT is that there's some number of buckets for which the data loss associated with the bucketing is perceptually negligible. However, the sophistication of the human auditory system, much like the visual cortex, may actually make it easier to produce perceptually negligible in many cases. Collins (2010) points out that the human ear can be tricked to identify a frequency in audio that doesn't exist when the primary frequency is removed from a signal that includes the primary frequencies harmonics.\n",
    "\n",
    "So the initial intuition is that we may be able to train a neural network to find the precise combination of frequencies that are interesting in a signal for performing a multi-label classification task.\n",
    "\n",
    "## Input size\n",
    "\n",
    "To add a dimension of challenge to having the neural network learn the DFT is that different frequencies require different input shapes for detection. Using a simple feed-forward network with a small input size should be able to capture high frequencies but low frequencies will be lost. And using a large input size will sufficiently capture lower frequencies in the signal, but may suffer from information loss if there is a lot of variance among the higher frequencies. So we have to add some complexity to address these issues.\n",
    "\n",
    "One way we may handle this is to just increase the size / complexity of hidden layers in the model so as to capture multiple time series events. Another approach may be to use an RNN on a very short input signal that can feed back into itself for lower frequencies. Another approach is to try to use signaling neural network layers.\n",
    "\n",
    "## Experiment\n",
    "\n",
    "This experiment is to explore these domains to train a model that simply labels specific frequencies present in a static noisy signal. This doesn't require a data set because the signals can be synthesized from randomly constructed labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fitting-karaoke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
      "Requirement already satisfied: torch==1.11 in /home/karagon/.local/lib/python3.10/site-packages (1.11.0+cu113)\n",
      "Requirement already satisfied: ipywidgets in /home/karagon/.local/lib/python3.10/site-packages (7.7.0)\n",
      "Requirement already satisfied: numpy in /usr/lib/python3/dist-packages (1.21.5)\n",
      "Requirement already satisfied: seaborn in /home/karagon/.local/lib/python3.10/site-packages (0.11.2)\n",
      "Requirement already satisfied: librosa in /home/karagon/.local/lib/python3.10/site-packages (0.9.1)\n",
      "Requirement already satisfied: tqdm in /home/karagon/.local/lib/python3.10/site-packages (4.64.0)\n",
      "Requirement already satisfied: torchvision in /home/karagon/.local/lib/python3.10/site-packages (0.12.0+cu113)\n",
      "Requirement already satisfied: torchaudio in /home/karagon/.local/lib/python3.10/site-packages (0.11.0+cu113)\n",
      "Requirement already satisfied: typing-extensions in /usr/lib/python3/dist-packages (from torch==1.11) (3.10.0.2)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /home/karagon/.local/lib/python3.10/site-packages (from ipywidgets) (1.1.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /home/karagon/.local/lib/python3.10/site-packages (from ipywidgets) (3.6.0)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /usr/lib/python3/dist-packages (from ipywidgets) (7.31.1)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /usr/lib/python3/dist-packages (from ipywidgets) (6.7.0)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/lib/python3/dist-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /usr/lib/python3/dist-packages (from ipywidgets) (5.1.3)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/lib/python3/dist-packages (from ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/lib/python3/dist-packages (from seaborn) (1.8.0)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /usr/lib/python3/dist-packages (from seaborn) (3.5.1)\n",
      "Requirement already satisfied: pandas>=0.23 in /home/karagon/.local/lib/python3.10/site-packages (from seaborn) (1.4.2)\n",
      "Requirement already satisfied: soundfile>=0.10.2 in /home/karagon/.local/lib/python3.10/site-packages (from librosa) (0.10.3.post1)\n",
      "Requirement already satisfied: joblib>=0.14 in /home/karagon/.local/lib/python3.10/site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: decorator>=4.0.10 in /usr/lib/python3/dist-packages (from librosa) (4.4.2)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /home/karagon/.local/lib/python3.10/site-packages (from librosa) (1.1.1)\n",
      "Requirement already satisfied: audioread>=2.1.5 in /home/karagon/.local/lib/python3.10/site-packages (from librosa) (2.1.9)\n",
      "Requirement already satisfied: numba>=0.45.1 in /home/karagon/.local/lib/python3.10/site-packages (from librosa) (0.55.2)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /home/karagon/.local/lib/python3.10/site-packages (from librosa) (0.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from librosa) (21.3)\n",
      "Requirement already satisfied: pooch>=1.0 in /home/karagon/.local/lib/python3.10/site-packages (from librosa) (1.6.0)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from torchvision) (2.25.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/lib/python3/dist-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /home/karagon/.local/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from numba>=0.45.1->librosa) (59.6.0)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /home/karagon/.local/lib/python3.10/site-packages (from numba>=0.45.1->librosa) (0.38.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=0.23->seaborn) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/lib/python3/dist-packages (from pandas>=0.23->seaborn) (2.8.1)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in /usr/lib/python3/dist-packages (from pooch>=1.0->librosa) (1.4.4)\n",
      "Requirement already satisfied: six>=1.3 in /usr/lib/python3/dist-packages (from resampy>=0.2.2->librosa) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/karagon/.local/lib/python3.10/site-packages (from scikit-learn>=0.19.1->librosa) (3.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/karagon/.local/lib/python3.10/site-packages (from soundfile>=0.10.2->librosa) (1.15.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /usr/lib/python3/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.4.8)\n",
      "Requirement already satisfied: pycparser in /home/karagon/.local/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.11 ipywidgets numpy seaborn librosa tqdm torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import librosa as lr\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown, Audio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-dayton",
   "metadata": {},
   "source": [
    "# Training and testing infrastructure\n",
    "\n",
    "First, the infrastructure for building data sets that we can use to test the model. \n",
    "\n",
    "## Label Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "least-circus",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GeneratedAudioDataSet(Dataset):\n",
    "    SAMPLE_LENGTH = 96000\n",
    "    FREQUENCY_COUNT = 48\n",
    "    LABELED_FREQUENCIES = np.array([49, 220, 392, 880, 1760, 2093, 3520, 7040, 22000])\n",
    "    FREQUENCY_LOOKUP = {k: i for i, k in enumerate(LABELED_FREQUENCIES)}\n",
    "    \n",
    "    def _freq(self, freq, offset):\n",
    "        start = offset / freq\n",
    "        end = self._il / freq\n",
    "        t = np.linspace(start, start + end, num=self._il) * math.pi * 2\n",
    "        return np.sin(t)\n",
    "    \n",
    "    def __init__(self, input_len, samples):\n",
    "        self._il = input_len\n",
    "      \n",
    "        random_predefined = GeneratedAudioDataSet.LABELED_FREQUENCIES[np.random.randint(\n",
    "            0, \n",
    "            len(GeneratedAudioDataSet.LABELED_FREQUENCIES),\n",
    "            size=(samples, GeneratedAudioDataSet.FREQUENCY_COUNT))]\n",
    "    \n",
    "        freqs = np.random.randint(10, high=24000, size=(samples, GeneratedAudioDataSet.FREQUENCY_COUNT, 2));  \n",
    "        use_predef = np.random.randint(0, 10, size=(samples, GeneratedAudioDataSet.FREQUENCY_COUNT)) == 0\n",
    "        \n",
    "        freqs[use_predef, 0] = random_predefined[use_predef]\n",
    "\n",
    "        amps = 1 - np.random.power(4, (samples, GeneratedAudioDataSet.FREQUENCY_COUNT, 1))\n",
    "        amps **= 2\n",
    "        self._sigs = np.concatenate([freqs.astype(np.float64), amps], axis=2)\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return self._il\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int((self._sigs.shape[0] * GeneratedAudioDataSet.SAMPLE_LENGTH) / self._il)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        idx = int(index / (96000 / self._il))\n",
    "        offset = (index % (96000 / self._il)) * self._il\n",
    "        \n",
    "        labels = np.zeros(len(GeneratedAudioDataSet.LABELED_FREQUENCIES), dtype=np.float32)\n",
    "        tone = np.zeros(self._il, dtype=np.float32)\n",
    "        for s in range(0, GeneratedAudioDataSet.FREQUENCY_COUNT):\n",
    "            tone += self._freq(self._sigs[idx, s, 0], offset + self._sigs[idx, s, 1]) * self._sigs[idx, s, 2]\n",
    "            if self._sigs[idx, s, 0] in GeneratedAudioDataSet.FREQUENCY_LOOKUP:\n",
    "                labels[GeneratedAudioDataSet.FREQUENCY_LOOKUP[self._sigs[idx, s, 0]]] = self._sigs[idx, s, 2]\n",
    "        \n",
    "        return tone, labels\n",
    "    \n",
    "def plot_and_display(ds, index=0):\n",
    "    sample_len = int(96000 / ds.batch_size)\n",
    "    sample = np.zeros(96000)\n",
    "    \n",
    "    start_basis = index * sample_len\n",
    "\n",
    "    for i in trange(sample_len):\n",
    "        st = i * ds.batch_size\n",
    "        sample[st:st+ds.batch_size] = ds[start_basis + i][0]\n",
    "\n",
    "    sns.lineplot(x = np.arange(sample.shape[0]), y=sample)\n",
    "    display(Audio(sample, rate=48000))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-plastic",
   "metadata": {},
   "source": [
    "# First pass\n",
    "\n",
    "After endless headbanging - I switch gears to see if I can reproduce the behaviour of endolith where he managed to train a model to learn the DFT with incredible accuracy, albeit with less efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adapted-kingdom",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bce86a4c3df453f86ffe387f15fc5a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "class MSLELoss(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss(*args, **kwargs)\n",
    "        \n",
    "    def forward(self, pred, actual):\n",
    "        return self.mse(torch.log(pred + 1), torch.log(actual + 1))\n",
    "\n",
    "# After trying to use a simple feed-forward network to simply learn\n",
    "# the amplitudes, I hit a brick wall at 65% accuracy. So before moving on,\n",
    "# I will build this simple model and the initial training set to validate\n",
    "# the work by endolith on creating an fft\n",
    "class LearningFFT(nn.Module):\n",
    "    \n",
    "    def _train_epoch(self, batches):\n",
    "        optimizer = optim.SGD(self.parameters(), lr=0.0001, momentum=0.99)\n",
    "        criterion = MSLELoss(reduction='sum')\n",
    "        \n",
    "        running_loss = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for i in range(batches):\n",
    "            sig = ((torch.rand(100, self._isize) - 0.5) * 2).to(self.device)\n",
    "            \n",
    "            f = torch.fft.fft(torch.cos(sig * math.pi * 2) + 1j * torch.sin(sig * math.pi * 2))\n",
    "            # f = torch.cat((f.real, f.imag), dim=1).to(self.device)\n",
    "            f = torch.abs(f).to(self.device)\n",
    "\n",
    "            # reset the optimizer\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self(sig)\n",
    "            loss = criterion(outputs, f)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # accumulate loss\n",
    "            running_loss += loss.item()\n",
    "            train_total += 1\n",
    "            \n",
    "        return running_loss / train_total\n",
    "        \n",
    "    def __init__(self, input_size, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._isize = input_size\n",
    "        self._trained = False\n",
    "\n",
    "        # simple first layer with no bias\n",
    "        self.signal_layer = nn.Linear(input_size * 2, input_size * 2, bias=False)\n",
    "        \n",
    "        init_epochs = 200\n",
    "        if 'fft_epochs' in kwargs:\n",
    "            init_epochs = kwargs['fft_epochs']\n",
    "            \n",
    "        self._fftepochs = init_epochs\n",
    "        self.train_fft()\n",
    "            \n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def train_fft(self):\n",
    "        self._trained = True\n",
    "        final_loss = 1e8\n",
    "        \n",
    "        for i in (ebar := trange(self._fftepochs)):\n",
    "            ebar.set_description(f'Training FFT epoch {i} of {self._fftepochs} current loss: {final_loss:.2e}')\n",
    "            final_loss = self._train_epoch(500)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if not self._trained:\n",
    "            raise ValueError(\"FFT is not yet trained... call train_fft\")\n",
    "        \n",
    "        x = x.to(self.device)\n",
    "        x = torch.mul(x, math.pi * 2)\n",
    "        \n",
    "        # convert the signal to the complex representation\n",
    "        x = torch.cat((torch.cos(x), torch.sin(x)), dim=1)\n",
    "        \n",
    "        # clamp the signal from -1 to 1\n",
    "        x = torch.clamp(x, min=-1, max=1)\n",
    "        \n",
    "        # the FFT is linear so this will be learned\n",
    "        x = self.signal_layer(x)\n",
    "        \n",
    "        #return x\n",
    "        \n",
    "        # re-flatten the real and imaginary\n",
    "        n = int(x.size(1) / 2)\n",
    "        x = torch.sqrt(torch.pow(x[:, :n], 2) + torch.pow(x[:, n:], 2))\n",
    "        return x;\n",
    "\n",
    "    \n",
    "fftmodel = LearningFFT(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-private",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test that we got a decent fft\n",
    "\n",
    "device = torch.device(str(\"cuda:0\") if torch.cuda.is_available() else \"cpu\")\n",
    "fftmodel.to(device)\n",
    "\n",
    "ds = GeneratedAudioDataSet(200, 1)\n",
    "\n",
    "start = time.process_time_ns()\n",
    "for i in range(0, 20, 4):\n",
    "    raw_signal = torch.tensor(ds[i][0] * 2 * math.pi)\n",
    "\n",
    "    yf = torch.fft.fft(torch.cos(raw_signal) + 1j*torch.sin(raw_signal))\n",
    "    yf = torch.abs(yf).numpy()\n",
    "    yf = yf[:int(len(yf)/2)]\n",
    "\n",
    "    sns.lineplot(x = np.arange(yf.shape[0]), y=yf)\n",
    "    \n",
    "end = time.process_time_ns()\n",
    "display(Markdown(f'Real Mathematical DFT ({(end - start) / 1000000.0} ms)'))\n",
    "plt.show()\n",
    "\n",
    "start = time.process_time_ns()\n",
    "for i in range(0, 20, 4):\n",
    "    yf = fftmodel(torch.tensor(ds[i][0].reshape((1, ds[i][0].shape[0]))))[0]\n",
    "    n = int(len(yf)/2)\n",
    "    #yf = torch.sqrt(torch.pow(yf[:n], 2) + torch.pow(yf[n:], 2))\n",
    "    yf = yf[:int(len(yf)/2)]\n",
    "    yf = yf.detach().cpu().numpy()\n",
    "    sns.lineplot(x = np.arange(yf.shape[0]), y=yf)\n",
    "\n",
    "end = time.process_time_ns()\n",
    "display(Markdown(f'Learned equivalent of DFT ({(end - start) / 1000000.0} ms)'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "funded-honey",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d88c3bc77804392a9ac2feee2fe853a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf22f89d415048b7a60607672f2563a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1280 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training Loss:   7.71585549004376\n",
      "Validation Loss: 22.093637299537658\n",
      "Accuracy:        0.021180555555555557\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13313424526841e88d14dac4927b8dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1280 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "Training Loss:   5.428861467353999\n",
      "Validation Loss: 21.945079731941224\n",
      "Accuracy:        0.5609027777777778\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "872574a025da4a068bcf37a24a091d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1280 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "Training Loss:   5.409874633140862\n",
      "Validation Loss: 21.911021900177\n",
      "Accuracy:        0.6281388888888889\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d193fedc0f824befbecebf1f117ceb9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1280 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n",
      "Training Loss:   5.404335951060057\n",
      "Validation Loss: 21.898151755332947\n",
      "Accuracy:        0.6281388888888889\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad75ed5ed7de41e8b435be82ecf94ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1280 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n",
      "Training Loss:   5.401944482699037\n",
      "Validation Loss: 21.891910338401793\n",
      "Accuracy:        0.6281388888888889\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af1c395d03b0443e93ae1cee2f6ad974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1280 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6\n",
      "Training Loss:   5.400779131986201\n",
      "Validation Loss: 21.88889081478119\n",
      "Accuracy:        0.6281388888888889\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ae527212fd41038225908b6e095f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1280 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7\n",
      "Training Loss:   5.40021761059761\n",
      "Validation Loss: 21.887450957298277\n",
      "Accuracy:        0.6281388888888889\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eeb17a01d154f05b36fa08e9c836788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1280 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8\n",
      "Training Loss:   5.3999286102131006\n",
      "Validation Loss: 21.88664174079895\n",
      "Accuracy:        0.6281388888888889\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28dc39a56c7543f8baf4cafb93ca97a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1280 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9\n",
      "Training Loss:   5.3996448187157515\n",
      "Validation Loss: 21.882446193695067\n",
      "Accuracy:        0.6224027777777777\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b54e85c58b4b47b68d5d5ca50a6717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1280 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10\n",
      "Training Loss:   5.372188764251769\n",
      "Validation Loss: 21.539423775672912\n",
      "Accuracy:        0.5996666666666667\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'train_loss': 7.71585549004376,\n",
       "  'valid_loss': 22.093637299537658,\n",
       "  'accuracy': 0.021180555555555557},\n",
       " {'train_loss': 5.428861467353999,\n",
       "  'valid_loss': 21.945079731941224,\n",
       "  'accuracy': 0.5609027777777778},\n",
       " {'train_loss': 5.409874633140862,\n",
       "  'valid_loss': 21.911021900177,\n",
       "  'accuracy': 0.6281388888888889},\n",
       " {'train_loss': 5.404335951060057,\n",
       "  'valid_loss': 21.898151755332947,\n",
       "  'accuracy': 0.6281388888888889},\n",
       " {'train_loss': 5.401944482699037,\n",
       "  'valid_loss': 21.891910338401793,\n",
       "  'accuracy': 0.6281388888888889},\n",
       " {'train_loss': 5.400779131986201,\n",
       "  'valid_loss': 21.88889081478119,\n",
       "  'accuracy': 0.6281388888888889},\n",
       " {'train_loss': 5.40021761059761,\n",
       "  'valid_loss': 21.887450957298277,\n",
       "  'accuracy': 0.6281388888888889},\n",
       " {'train_loss': 5.3999286102131006,\n",
       "  'valid_loss': 21.88664174079895,\n",
       "  'accuracy': 0.6281388888888889},\n",
       " {'train_loss': 5.3996448187157515,\n",
       "  'valid_loss': 21.882446193695067,\n",
       "  'accuracy': 0.6224027777777777},\n",
       " {'train_loss': 5.372188764251769,\n",
       "  'valid_loss': 21.539423775672912,\n",
       "  'accuracy': 0.5996666666666667}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FrequencyFitModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._isize = input_size\n",
    "        \n",
    "        self.freq_layer = LearningFFT(input_size, **kwargs)\n",
    "        self.batch_norm = nn.BatchNorm1d(input_size)\n",
    "        self.hidden_layer = nn.Linear(input_size, 64, bias=False)\n",
    "        self.output_layer = nn.Linear(64, len(GeneratedAudioDataSet.LABELED_FREQUENCIES))\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.freq_layer.device\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)\n",
    "        x = self.freq_layer(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        x = self.hidden_layer(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.output_layer(x)\n",
    "        return torch.sigmoid(x);\n",
    "    \n",
    "def fit_model(model, ds, **kwargs):\n",
    "    train_size = int(0.8 * len(ds))\n",
    "    validation_size = len(ds) - train_size\n",
    "    train_dataset, validation_dataset = torch.utils.data.random_split(ds, [train_size, validation_size])\n",
    "\n",
    "    batch_size=16\n",
    "    if 'batch_size' in kwargs:\n",
    "        batch_size = kwargs['batch_size']\n",
    "    \n",
    "    epochs = 10\n",
    "    if 'epochs' in kwargs:\n",
    "        epochs = kwargs['epochs']\n",
    "    \n",
    "    criterion = None\n",
    "    if 'loss' in kwargs:\n",
    "        criterion = kwargs['loss']\n",
    "    else:\n",
    "        criterion = nn.BCELoss()\n",
    "        \n",
    "    optimizer = None\n",
    "    if 'optimizer' in kwargs:\n",
    "        optimizer = kwargs['optimizer']\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters())\n",
    "\n",
    "    loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(validation_dataset, batch_size=100)\n",
    "        \n",
    "    epoch_stats = []\n",
    "    for epoch in range(epochs):\n",
    "    \n",
    "        # track total loss\n",
    "        running_loss = 0.0\n",
    "        train_total = 0\n",
    "        \n",
    "        # training\n",
    "        for batch in tqdm(loader):\n",
    "            X, Y = batch\n",
    "\n",
    "            # zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X.to(model.device))\n",
    "            loss = criterion(outputs, Y.to(model.device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # accumulate loss\n",
    "            running_loss += loss.item()\n",
    "            train_total += 1\n",
    "         \n",
    "        # validation\n",
    "        with torch.no_grad():\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            label_total = 0\n",
    "            valid_loss = 0.0\n",
    "            \n",
    "            for batch in valid_loader:\n",
    "                X, Y = batch\n",
    "                X = X.to(model.device)\n",
    "                Y = Y.to(model.device)\n",
    "                \n",
    "                outputs = model(X)\n",
    "                \n",
    "                loss = criterion(outputs, Y)\n",
    "                valid_loss += loss.item()\n",
    "                correct += (torch.round(outputs, decimals=3) == torch.round(Y, decimals=3)).sum().item()\n",
    "                total += 1\n",
    "                label_total += Y.nelement()\n",
    "\n",
    "            epoch_stats.append({\n",
    "                'train_loss': running_loss / train_total,\n",
    "                'valid_loss': valid_loss / total,\n",
    "                'accuracy': correct / label_total\n",
    "            })\n",
    "            \n",
    "            print(f'Epoch {epoch + 1}')\n",
    "            print(f'Training Loss:   {epoch_stats[-1][\"train_loss\"]}')\n",
    "            print(f'Validation Loss: {epoch_stats[-1][\"valid_loss\"]}')\n",
    "            print(f'Accuracy:        {epoch_stats[-1][\"accuracy\"]}')\n",
    "            print()\n",
    "            \n",
    "    return epoch_stats\n",
    "\n",
    "ds = GeneratedAudioDataSet(240, 100)\n",
    "model = FrequencyFitModel(ds.batch_size, fft_epochs=35)\n",
    "model.to(device)\n",
    "\n",
    "fit_model(\n",
    "    model, \n",
    "    ds,\n",
    "    batch_size=25,\n",
    "    loss=nn.L1Loss(reduction='sum'),\n",
    "    optimizer=optim.NAdam(model.parameters(), lr=0.001),\n",
    "    epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-handle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isp",
   "language": "python",
   "name": "isp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
